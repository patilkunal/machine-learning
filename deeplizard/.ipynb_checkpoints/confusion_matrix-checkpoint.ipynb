{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i,j], horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own data here of imaginary clinical trial\n",
    "for i in range(50):\n",
    "    # The ~5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1) # denotes they did experiences side effects\n",
    "\n",
    "    # The ~5% of older individuals who did NOT experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0) # denotes they did NOT experiences side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0) # denotes they did NOT experiences side effects\n",
    "\n",
    "    # The ~95% of older individuals who did NOT experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1) # denotes they did experiences side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDTIONAL CODE to validation_set.py\n",
    "test_labels = []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own data here of imaginary clinical trial\n",
    "for i in range(10):\n",
    "    # The ~5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1) # denotes they did experiences side effects\n",
    "\n",
    "    # The ~5% of older individuals who did NOT experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0) # denotes they did NOT experiences side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    # The ~95% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0) # denotes they did NOT experiences side effects\n",
    "\n",
    "    # The ~95% of older individuals who did NOT experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1) # denotes they did experiences side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now process above data\n",
    "\n",
    "# Make them as numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "# Shuffle them to make them random\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "# Shuffle them to make them random\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the age data to make them in range of 0 to 1 (as against 13 - 100)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1, 1))\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just print scaled data\n",
    "# for i in scaled_train_samples:\n",
    "#    print(i)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2,  activation='softmax') # units = 2 since we need two outputs (did or did not experience )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile it\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/57\n",
      "189/189 - 0s - loss: 0.7172 - accuracy: 0.4921 - val_loss: 0.7039 - val_accuracy: 0.4952\n",
      "Epoch 2/57\n",
      "189/189 - 0s - loss: 0.6886 - accuracy: 0.7127 - val_loss: 0.6753 - val_accuracy: 0.8333\n",
      "Epoch 3/57\n",
      "189/189 - 0s - loss: 0.6649 - accuracy: 0.8280 - val_loss: 0.6517 - val_accuracy: 0.8333\n",
      "Epoch 4/57\n",
      "189/189 - 0s - loss: 0.6438 - accuracy: 0.8212 - val_loss: 0.6305 - val_accuracy: 0.8524\n",
      "Epoch 5/57\n",
      "189/189 - 0s - loss: 0.6227 - accuracy: 0.8296 - val_loss: 0.6074 - val_accuracy: 0.8333\n",
      "Epoch 6/57\n",
      "189/189 - 0s - loss: 0.5999 - accuracy: 0.8302 - val_loss: 0.5827 - val_accuracy: 0.8524\n",
      "Epoch 7/57\n",
      "189/189 - 0s - loss: 0.5752 - accuracy: 0.8423 - val_loss: 0.5561 - val_accuracy: 0.8524\n",
      "Epoch 8/57\n",
      "189/189 - 0s - loss: 0.5486 - accuracy: 0.8429 - val_loss: 0.5271 - val_accuracy: 0.8714\n",
      "Epoch 9/57\n",
      "189/189 - 0s - loss: 0.5205 - accuracy: 0.8545 - val_loss: 0.4975 - val_accuracy: 0.8810\n",
      "Epoch 10/57\n",
      "189/189 - 0s - loss: 0.4922 - accuracy: 0.8635 - val_loss: 0.4676 - val_accuracy: 0.8905\n",
      "Epoch 11/57\n",
      "189/189 - 0s - loss: 0.4646 - accuracy: 0.8799 - val_loss: 0.4395 - val_accuracy: 0.9000\n",
      "Epoch 12/57\n",
      "189/189 - 0s - loss: 0.4387 - accuracy: 0.8862 - val_loss: 0.4136 - val_accuracy: 0.9143\n",
      "Epoch 13/57\n",
      "189/189 - 0s - loss: 0.4154 - accuracy: 0.8974 - val_loss: 0.3901 - val_accuracy: 0.9143\n",
      "Epoch 14/57\n",
      "189/189 - 0s - loss: 0.3945 - accuracy: 0.9000 - val_loss: 0.3694 - val_accuracy: 0.9238\n",
      "Epoch 15/57\n",
      "189/189 - 0s - loss: 0.3763 - accuracy: 0.9053 - val_loss: 0.3511 - val_accuracy: 0.9238\n",
      "Epoch 16/57\n",
      "189/189 - 0s - loss: 0.3606 - accuracy: 0.9063 - val_loss: 0.3355 - val_accuracy: 0.9286\n",
      "Epoch 17/57\n",
      "189/189 - 0s - loss: 0.3470 - accuracy: 0.9116 - val_loss: 0.3219 - val_accuracy: 0.9286\n",
      "Epoch 18/57\n",
      "189/189 - 0s - loss: 0.3352 - accuracy: 0.9132 - val_loss: 0.3103 - val_accuracy: 0.9286\n",
      "Epoch 19/57\n",
      "189/189 - 0s - loss: 0.3254 - accuracy: 0.9143 - val_loss: 0.3002 - val_accuracy: 0.9429\n",
      "Epoch 20/57\n",
      "189/189 - 0s - loss: 0.3172 - accuracy: 0.9206 - val_loss: 0.2921 - val_accuracy: 0.9429\n",
      "Epoch 21/57\n",
      "189/189 - 0s - loss: 0.3101 - accuracy: 0.9228 - val_loss: 0.2850 - val_accuracy: 0.9429\n",
      "Epoch 22/57\n",
      "189/189 - 0s - loss: 0.3042 - accuracy: 0.9254 - val_loss: 0.2793 - val_accuracy: 0.9429\n",
      "Epoch 23/57\n",
      "189/189 - 0s - loss: 0.2991 - accuracy: 0.9228 - val_loss: 0.2738 - val_accuracy: 0.9476\n",
      "Epoch 24/57\n",
      "189/189 - 0s - loss: 0.2949 - accuracy: 0.9291 - val_loss: 0.2694 - val_accuracy: 0.9476\n",
      "Epoch 25/57\n",
      "189/189 - 0s - loss: 0.2911 - accuracy: 0.9286 - val_loss: 0.2657 - val_accuracy: 0.9476\n",
      "Epoch 26/57\n",
      "189/189 - 0s - loss: 0.2880 - accuracy: 0.9291 - val_loss: 0.2626 - val_accuracy: 0.9476\n",
      "Epoch 27/57\n",
      "189/189 - 0s - loss: 0.2850 - accuracy: 0.9312 - val_loss: 0.2599 - val_accuracy: 0.9476\n",
      "Epoch 28/57\n",
      "189/189 - 0s - loss: 0.2824 - accuracy: 0.9270 - val_loss: 0.2571 - val_accuracy: 0.9476\n",
      "Epoch 29/57\n",
      "189/189 - 0s - loss: 0.2807 - accuracy: 0.9307 - val_loss: 0.2551 - val_accuracy: 0.9476\n",
      "Epoch 30/57\n",
      "189/189 - 0s - loss: 0.2785 - accuracy: 0.9328 - val_loss: 0.2530 - val_accuracy: 0.9476\n",
      "Epoch 31/57\n",
      "189/189 - 0s - loss: 0.2766 - accuracy: 0.9312 - val_loss: 0.2513 - val_accuracy: 0.9476\n",
      "Epoch 32/57\n",
      "189/189 - 0s - loss: 0.2750 - accuracy: 0.9296 - val_loss: 0.2496 - val_accuracy: 0.9476\n",
      "Epoch 33/57\n",
      "189/189 - 0s - loss: 0.2736 - accuracy: 0.9365 - val_loss: 0.2484 - val_accuracy: 0.9476\n",
      "Epoch 34/57\n",
      "189/189 - 0s - loss: 0.2719 - accuracy: 0.9312 - val_loss: 0.2468 - val_accuracy: 0.9476\n",
      "Epoch 35/57\n",
      "189/189 - 0s - loss: 0.2706 - accuracy: 0.9354 - val_loss: 0.2457 - val_accuracy: 0.9476\n",
      "Epoch 36/57\n",
      "189/189 - 0s - loss: 0.2694 - accuracy: 0.9312 - val_loss: 0.2447 - val_accuracy: 0.9476\n",
      "Epoch 37/57\n",
      "189/189 - 0s - loss: 0.2684 - accuracy: 0.9360 - val_loss: 0.2437 - val_accuracy: 0.9476\n",
      "Epoch 38/57\n",
      "189/189 - 0s - loss: 0.2672 - accuracy: 0.9339 - val_loss: 0.2428 - val_accuracy: 0.9476\n",
      "Epoch 39/57\n",
      "189/189 - 0s - loss: 0.2661 - accuracy: 0.9317 - val_loss: 0.2420 - val_accuracy: 0.9524\n",
      "Epoch 40/57\n",
      "189/189 - 0s - loss: 0.2653 - accuracy: 0.9386 - val_loss: 0.2411 - val_accuracy: 0.9524\n",
      "Epoch 41/57\n",
      "189/189 - 0s - loss: 0.2641 - accuracy: 0.9407 - val_loss: 0.2404 - val_accuracy: 0.9476\n",
      "Epoch 42/57\n",
      "189/189 - 0s - loss: 0.2634 - accuracy: 0.9365 - val_loss: 0.2398 - val_accuracy: 0.9476\n",
      "Epoch 43/57\n",
      "189/189 - 0s - loss: 0.2627 - accuracy: 0.9386 - val_loss: 0.2391 - val_accuracy: 0.9476\n",
      "Epoch 44/57\n",
      "189/189 - 0s - loss: 0.2617 - accuracy: 0.9397 - val_loss: 0.2387 - val_accuracy: 0.9476\n",
      "Epoch 45/57\n",
      "189/189 - 0s - loss: 0.2610 - accuracy: 0.9376 - val_loss: 0.2379 - val_accuracy: 0.9524\n",
      "Epoch 46/57\n",
      "189/189 - 0s - loss: 0.2601 - accuracy: 0.9344 - val_loss: 0.2374 - val_accuracy: 0.9524\n",
      "Epoch 47/57\n",
      "189/189 - 0s - loss: 0.2594 - accuracy: 0.9381 - val_loss: 0.2369 - val_accuracy: 0.9524\n",
      "Epoch 48/57\n",
      "189/189 - 0s - loss: 0.2586 - accuracy: 0.9413 - val_loss: 0.2363 - val_accuracy: 0.9524\n",
      "Epoch 49/57\n",
      "189/189 - 0s - loss: 0.2580 - accuracy: 0.9423 - val_loss: 0.2359 - val_accuracy: 0.9524\n",
      "Epoch 50/57\n",
      "189/189 - 0s - loss: 0.2574 - accuracy: 0.9397 - val_loss: 0.2353 - val_accuracy: 0.9524\n",
      "Epoch 51/57\n",
      "189/189 - 0s - loss: 0.2567 - accuracy: 0.9423 - val_loss: 0.2348 - val_accuracy: 0.9524\n",
      "Epoch 52/57\n",
      "189/189 - 0s - loss: 0.2560 - accuracy: 0.9418 - val_loss: 0.2343 - val_accuracy: 0.9524\n",
      "Epoch 53/57\n",
      "189/189 - 0s - loss: 0.2555 - accuracy: 0.9423 - val_loss: 0.2339 - val_accuracy: 0.9524\n",
      "Epoch 54/57\n",
      "189/189 - 0s - loss: 0.2549 - accuracy: 0.9407 - val_loss: 0.2334 - val_accuracy: 0.9524\n",
      "Epoch 55/57\n",
      "189/189 - 0s - loss: 0.2543 - accuracy: 0.9418 - val_loss: 0.2328 - val_accuracy: 0.9524\n",
      "Epoch 56/57\n",
      "189/189 - 0s - loss: 0.2538 - accuracy: 0.9423 - val_loss: 0.2324 - val_accuracy: 0.9524\n",
      "Epoch 57/57\n",
      "189/189 - 0s - loss: 0.2535 - accuracy: 0.9407 - val_loss: 0.2320 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2639dbadd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it and split 10% of data as validation set\n",
    "# even though shuffle is true, validation set is seperated before shuffle, so it may not contain random data as we want\n",
    "# we will see val_loss & val_accuracy output as against when we did not specify the validation split param\n",
    "model.fit(x=scaled_train_samples, y=train_labels, validation_split=0.1, batch_size=10, epochs=39, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW PREDICT using test samples\n",
    "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0)\n",
    "rounded_predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['No Side Effects', 'Side Effects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is file already exist and save if not\n",
    "import os.path\n",
    "filename = 'models/medical_trial_model.h5'\n",
    "if os.path.isfile(filename) is False:\n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This saves:\n",
    " \n",
    "    * The arch of the model which allows to re-create the model\n",
    "    * The weights of the model\n",
    "    * The training configuration (loss, optimizer)\n",
    "    * The state of the optimizer, allowing to resume training exactly where you left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
